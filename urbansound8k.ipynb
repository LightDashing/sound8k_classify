{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ESR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tj07_ds8Kmq"
      },
      "source": [
        "First of all, we need to install kaggle api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG0UcqsbmhlP"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Z0ZvKr8R46"
      },
      "source": [
        "Then, download dataset with sounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss63FFh9nfWB"
      },
      "source": [
        "!kaggle datasets download -d chrisfilo/urbansound8k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG53JVfv8Y1d"
      },
      "source": [
        "Unpacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3o7GrSNoAoF"
      },
      "source": [
        "!unzip urbansound8k.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhESYSGv8a3s"
      },
      "source": [
        "Deleting archive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klG960Cwr4AW"
      },
      "source": [
        "!rm urbansound8k.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP2Zz8fH8f_Y"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxBmFQTl4D4H"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.plotting.register_matplotlib_converters()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biKg_awx4G1M"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout, BatchNormalization, Conv1D, MaxPooling1D, SeparableConv2D, Input, LSTM, Activation\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgikxygw4ILi"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display \n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF6OYQnn8hnL"
      },
      "source": [
        "Reading dataframe with folds and classIDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HLu3FdK9fAGh",
        "outputId": "8656c469-a635-414d-82b2-e113e9cfe0ba"
      },
      "source": [
        "df = pd.read_csv(\"UrbanSound8K.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slice_file_name</th>\n",
              "      <th>fsID</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>salience</th>\n",
              "      <th>fold</th>\n",
              "      <th>classID</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100032-3-0-0.wav</td>\n",
              "      <td>100032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.317551</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>dog_bark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100263-2-0-117.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>62.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100263-2-0-121.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>60.500000</td>\n",
              "      <td>64.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100263-2-0-126.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100263-2-0-137.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>68.500000</td>\n",
              "      <td>72.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8727</th>\n",
              "      <td>99812-1-2-0.wav</td>\n",
              "      <td>99812</td>\n",
              "      <td>159.522205</td>\n",
              "      <td>163.522205</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>car_horn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8728</th>\n",
              "      <td>99812-1-3-0.wav</td>\n",
              "      <td>99812</td>\n",
              "      <td>181.142431</td>\n",
              "      <td>183.284976</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>car_horn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8729</th>\n",
              "      <td>99812-1-4-0.wav</td>\n",
              "      <td>99812</td>\n",
              "      <td>242.691902</td>\n",
              "      <td>246.197885</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>car_horn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8730</th>\n",
              "      <td>99812-1-5-0.wav</td>\n",
              "      <td>99812</td>\n",
              "      <td>253.209850</td>\n",
              "      <td>255.741948</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>car_horn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8731</th>\n",
              "      <td>99812-1-6-0.wav</td>\n",
              "      <td>99812</td>\n",
              "      <td>332.289233</td>\n",
              "      <td>334.821332</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>car_horn</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8732 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         slice_file_name    fsID       start  ...  fold  classID             class\n",
              "0       100032-3-0-0.wav  100032    0.000000  ...     5        3          dog_bark\n",
              "1     100263-2-0-117.wav  100263   58.500000  ...     5        2  children_playing\n",
              "2     100263-2-0-121.wav  100263   60.500000  ...     5        2  children_playing\n",
              "3     100263-2-0-126.wav  100263   63.000000  ...     5        2  children_playing\n",
              "4     100263-2-0-137.wav  100263   68.500000  ...     5        2  children_playing\n",
              "...                  ...     ...         ...  ...   ...      ...               ...\n",
              "8727     99812-1-2-0.wav   99812  159.522205  ...     7        1          car_horn\n",
              "8728     99812-1-3-0.wav   99812  181.142431  ...     7        1          car_horn\n",
              "8729     99812-1-4-0.wav   99812  242.691902  ...     7        1          car_horn\n",
              "8730     99812-1-5-0.wav   99812  253.209850  ...     7        1          car_horn\n",
              "8731     99812-1-6-0.wav   99812  332.289233  ...     7        1          car_horn\n",
              "\n",
              "[8732 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pbENxePXVc9",
        "outputId": "5cdaaa07-693b-422b-d34a-6697661f93c9"
      },
      "source": [
        "df[\"class\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dog_bark            1000\n",
              "street_music        1000\n",
              "engine_idling       1000\n",
              "air_conditioner     1000\n",
              "jackhammer          1000\n",
              "drilling            1000\n",
              "children_playing    1000\n",
              "siren                929\n",
              "car_horn             429\n",
              "gun_shot             374\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poOv8JgXVgUB",
        "outputId": "6d5c701c-1a33-44fc-ed92-00883b7c5b4c"
      },
      "source": [
        "y, sr = librosa.load(\"air_conditioner.wav\", sr=22050)\n",
        "mfcc = np.mean(librosa.feature.mfcc(y, sr, n_mfcc=100).T, axis=0)\n",
        "mfcc = mfcc.reshape((1, 20, 5))\n",
        "predicts = model.predict(mfcc)\n",
        "cl = np.argmax(predicts, axis=1)\n",
        "classes = df[[\"classID\", \"class\"]]\n",
        "classes[classes.classID == cl[0]].iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "classID           3\n",
              "class      dog_bark\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1GUWrM98m9V"
      },
      "source": [
        "On this stage I'm augmenting data using pitch shift and time stretch, simultaneously getting mfcc (mel frequency cepstral coefficient) with n_mfcc=100. Because of very large amount of time to process different features (such as mel spectrogram, chroma stft and etc.) I'm using only mfcc for classification task. It's worth noting that most often mfcc gives the greatest accuracy, relative to all other features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BznWR4SkaAku"
      },
      "source": [
        "folds = [\"x\", \"fold1/\", \"fold2/\", \"fold3/\", \"fold4/\", \"fold5/\", \"fold6/\", \"fold7/\", \"fold8/\", \"fold9/\", \"fold10/\"]\n",
        "savedir = '/content/'\n",
        "\n",
        "def data_processing(fold):\n",
        "  mfcc = []\n",
        "  files = df[df[\"fold\"] == fold]\n",
        "  labels = []\n",
        "  def _audio_features(y, sr):\n",
        "    mfcc.append(np.mean(librosa.feature.mfcc(y, sr, n_mfcc=100).T, axis=0))\n",
        "\n",
        "  fold = f\"fold{fold}/\"\n",
        "  for wav in tqdm(os.listdir(savedir+fold)):\n",
        "    label = files[files['slice_file_name'] == wav]['classID']\n",
        "    y, sr = librosa.load(savedir+fold+wav)\n",
        "    _audio_features(y, sr)\n",
        "    labels.append(label.iloc[0]) \n",
        "    for i in [-2, -1, 1, 2]:\n",
        "      y_aug = librosa.effects.pitch_shift(y, sr, i)\n",
        "      _audio_features(y_aug, sr)\n",
        "      labels.append(label.iloc[0])\n",
        "    for i in [0.9, 1.1]:\n",
        "      y_aug = librosa.effects.time_stretch(y, i)\n",
        "      _audio_features(y_aug, sr)\n",
        "      labels.append(label.iloc[0])\n",
        "    for i in [-2, -1, 1, 2]:\n",
        "      for j in [0.9, 1.1]:\n",
        "        y_aug = librosa.effects.pitch_shift(y, sr, i)\n",
        "        y_aug = librosa.effects.time_stretch(y_aug, j)\n",
        "        _audio_features(y_aug, sr)\n",
        "        labels.append(label.iloc[0])\n",
        "\n",
        "  return mfcc, labels\n",
        "\n",
        "save_dir = \"/gdrive/MyDrive/ESCData/\"\n",
        "for i in range(10, 11):\n",
        "  mfcc, labels = data_processing(i)  \n",
        "  mfcc, labels = np.array(mfcc), np.array(labels)\n",
        "  np.savez(f\"{save_dir}{folds[i]}mfcc\", mfcc)\n",
        "  np.savez(f\"{save_dir}{folds[i]}labels\", labels)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD7hdKLRAKlr"
      },
      "source": [
        "After augmentation and processing, I have 10 different files with labels and mfcc on my google drive. Processing even one fold takes about 40 minutes, so it's better to save processed data at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tTk-RzIAnuV"
      },
      "source": [
        "(I need to write how many data I get from augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzbC8-ZDNzll"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "loaddir = '/content/drive/MyDrive/ESCData/'\n",
        "folds = [\"fold1/\", \"fold2/\", \"fold3/\", \"fold4/\", \"fold5/\", \"fold6/\", \"fold7/\", \"fold8/\", \"fold9/\", \"fold10/\"]\n",
        "x_train = []\n",
        "y_train = []\n",
        "for i in range(1, 9):\n",
        "  data = np.load(loaddir+folds[i]+\"mfcc.npz\", allow_pickle=True)\n",
        "  labels = np.load(loaddir+folds[i]+\"labels.npz\", allow_pickle=True)\n",
        "  x_train.append(data[\"arr_0\"])\n",
        "  y_train.append(labels[\"arr_0\"])\n",
        "\n",
        "x_test = np.load(loaddir+\"fold10/\"+\"mfcc.npz\", allow_pickle=True)[\"arr_0\"]\n",
        "x_test = x_test.reshape((x_test.shape[0], 20, 5))\n",
        "y_test = np.load(loaddir+\"fold10/\"+\"labels.npz\", allow_pickle=True)[\"arr_0\"]\n",
        "\n",
        "x_train = np.array(x_train, dtype='object')\n",
        "x_train = np.concatenate(x_train, axis=0).astype(np.float32)\n",
        "x_train = x_train.reshape((x_train.shape[0], 20, 5))\n",
        "\n",
        "y_train = np.array(y_train, dtype='object')\n",
        "y_train = np.concatenate(y_train, axis = 0).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTxVLMez_xU3"
      },
      "source": [
        "For using with StratifiedKFold, I need all data in one piece\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gR8lQAkx5gO"
      },
      "source": [
        "x = np.concatenate([x_train, x_test], axis = 0)\n",
        "y = np.concatenate([y_train, y_test], axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0XI2LGuO3u8"
      },
      "source": [
        "model = keras.models.load_model(\"/content/drive/MyDrive/ESCData/esc_model9.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt-4SLITMolI"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "acc = []\n",
        "i = 0\n",
        "\n",
        "for train_ind, test_ind in tqdm(fold.split(x, y)):\n",
        "  x_train, y_train = x[train_ind], y[train_ind]\n",
        "  x_test, y_test = x[test_ind], y[test_ind]\n",
        "\n",
        "  y_train = to_categorical(y_train, num_classes=10)\n",
        "  y_test = to_categorical(y_test, num_classes=10)\n",
        "  \n",
        "  checkpoint = ModelCheckpoint(f\"{model_savedir}esc_model{i}.hdf5\", save_best_only=True)\n",
        "  model = create_model()\n",
        "  model.fit(x_train, y_train, batch_size=512, epochs=50, verbose=0, callbacks=[checkpoint], validation_data=(x_test, y_test))\n",
        "  acc.append(model.evaluate(x_test, y_test))\n",
        "  i += 1\n",
        "\n",
        "model_savedir = '/content/drive/MyDrive/ESCData/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmOBDdIT_3fq"
      },
      "source": [
        "StratifiedKFold results.</br>\n",
        "Feed-forward: 81.5%</br>\n",
        "LSTM: 99%</br>\n",
        "CNN: 98.6$</br>\n",
        "Results are much better with stratified fold and shuffle because of better data distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bvWT0iKA1Y0"
      },
      "source": [
        "There I'm using default folds, that was integrated into dataset, without any modifications. Because of bad data distribution, accuracy of all estimators much lower than with stratified k-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0U5MAYkvTiN"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "loaddir = '/content/drive/MyDrive/ESCData/'\n",
        "folds = [\"fold1/\", \"fold2/\", \"fold3/\", \"fold4/\", \"fold5/\", \"fold6/\", \"fold7/\", \"fold8/\", \"fold9/\", \"fold10/\"]\n",
        "\n",
        "ind_folds = [i for i in range(1, 11)]\n",
        "fold = KFold(10)\n",
        "accuracy = []\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(patience=3)\n",
        "stopping = EarlyStopping(patience=6)\n",
        "\n",
        "for create_neural in [create_lstm, create_model, create_feedforward]:\n",
        "  for train_ind, test_ind in fold.split(ind_folds):\n",
        "    \n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    for i in train_ind:\n",
        "      train_x.append(np.load(loaddir+folds[i]+\"mfcc.npz\", allow_pickle=True)[\"arr_0\"])\n",
        "      train_y.append(np.load(loaddir+folds[i]+\"labels.npz\", allow_pickle=True)[\"arr_0\"])\n",
        "\n",
        "    x_train = np.array(train_x, dtype='object')\n",
        "    x_train = np.concatenate(x_train, axis=0).astype(np.float32)\n",
        "    x_train = x_train.reshape((x_train.shape[0], 20, 5))\n",
        "\n",
        "    y_train = np.array(train_y, dtype='object')\n",
        "    y_train = np.concatenate(y_train, axis = 0).astype(np.float32)\n",
        "    y_train = to_categorical(y_train, num_classes=10)\n",
        "\n",
        "    test_x = np.load(loaddir+folds[test_ind[0]]+\"mfcc.npz\", allow_pickle=True)[\"arr_0\"]\n",
        "    x_test = test_x.reshape((test_x.shape[0], 20, 5))\n",
        "    test_y = np.load(loaddir+folds[test_ind[0]]+\"labels.npz\", allow_pickle=True)[\"arr_0\"]\n",
        "    y_test = to_categorical(test_y, num_classes=10)\n",
        "\n",
        "    model = create_neural()\n",
        "    model.fit(x_train, y_train, epochs = 50, batch_size = 512, validation_data=(x_test, y_test), callbacks=[reduce_lr, stopping], verbose=0)\n",
        "    accuracy.append(model.evaluate(x_test, y_test))\n",
        "\n",
        "accuracy = np.array(accuracy)\n",
        "np.mean(accuracy[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bC5bqkf27oM"
      },
      "source": [
        "After several tests, I compared accuracy of models on default folds</br>\n",
        "LSTM: 55.7%</br>\n",
        "CNN: 62.7%</br>\n",
        "Feed-forward: 55.9%</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_kuIvFzAg-z"
      },
      "source": [
        "As you can see, CNN is better performing on default data distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywg0Mf1VAyrq"
      },
      "source": [
        "Feed-forward network composed from two dense layers with relu activation, one with 256 neurons and second with 512 neurons.</br>\n",
        "In every model I'm using Adam optimizer with lr=1e-3, because models with that lr shows better results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVmYhWjaCCPM"
      },
      "source": [
        "input_shape = (100, )\n",
        "def create_feedforward():\n",
        "  input_shape = (100, )\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, activation='relu', input_shape=input_shape))\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer = keras.optimizers.Adam(lr=1e-3), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiracJ9qBHDN"
      },
      "source": [
        "LSTM network is more complex. It contain two LSTM layers, with 128 and 64 units respectively.</br>\n",
        "After all layers I'm using batch normalization to standardize the inputs to a next layer. It helps model to generalize and accelerates training.</br>\n",
        "After batch normalization layer I'm using dropout, starting from 0.3 and ending with 0.5, these layers preventing model from overfitting. At each training stage, individual nodes (neurons) are either dropped out of the net with probability 1-p or kept with probability p.</br>\n",
        "After LSTM layer comes two dense layers in time distributed wrapper, with 256 and 512 units accordingly. This wrapper allows to apply a layer to every temporal slice of an input.</br>\n",
        "After time distributed comes three last layers: one flatten and two dense, one with relu and other, which is last, with softmax. Flatten layer is used to make input from 4d to 1d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEpQAIobWKUp",
        "outputId": "0c9f1fed-9eb0-44e1-b663-57e0f2b457d2"
      },
      "source": [
        "input_shape = (20, 5)\n",
        "def create_lstm():\n",
        "  input_shape = (20, 5)\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
        "  model.add(keras.layers.TimeDistributed(layer=BatchNormalization()))\n",
        "  model.add(LSTM(64, return_sequences=True))\n",
        "  model.add(keras.layers.TimeDistributed(layer=BatchNormalization()))\n",
        "  model.add(keras.layers.TimeDistributed(layer=Dropout(0.3)))\n",
        "\n",
        "  model.add(keras.layers.TimeDistributed(layer=Dense(256, activation='relu')))\n",
        "  model.add(keras.layers.TimeDistributed(layer=BatchNormalization()))\n",
        "  model.add(keras.layers.TimeDistributed(layer=Dropout(0.4)))\n",
        "\n",
        "  model.add(keras.layers.TimeDistributed(layer=Dense(512, activation='relu')))\n",
        "  model.add(keras.layers.TimeDistributed(layer=BatchNormalization()))\n",
        "  model.add(keras.layers.TimeDistributed(layer=Dropout(0.4)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "  model.add(Dropout(0.5)) \n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer = keras.optimizers.Adam(lr=1e-3), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = create_lstm()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 20, 128)           68608     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 20, 128)           512       \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 64)            49408     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 20, 64)            256       \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 20, 256)           16640     \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 20, 256)           1024      \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 20, 512)           131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 20, 512)           2048      \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 20, 512)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10240)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               2621696   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 2,894,346\n",
            "Trainable params: 2,892,426\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phA4wn37CO0t"
      },
      "source": [
        "def create_model():\n",
        "  input_dim = (20, 5, 1)\n",
        "  model = Sequential()\n",
        "  model.add(Input(input_dim))\n",
        "  model.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
        "  model.add(keras.layers.SpatialDropout2D(0.2))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(128, (5, 5), padding='same', activation='relu'))\n",
        "  model.add(keras.layers.SpatialDropout2D(0.2))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "  model.add(Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "  model.add(keras.layers.Dropout(0.4))\n",
        "  model.add(Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(Dense(10, activation = \"softmax\"))\n",
        "  model.compile(optimizer = keras.optimizers.Adam(lr=1e-3), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}